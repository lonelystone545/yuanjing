1 zk应用：集群管理 配置管理 命名服务 分布式锁 队列管理
	配置管理：当程序分散部署在多台机器上时，要逐个改变配置就比较困难。将这些配置全部放到zk上，保存在zk的某个目录节点中，
		然后相关的应用程序对这个目录节点进行监听，一旦配置信息发生变化，每个应用就会收到zk的通知，然后从zk获取新的配置信息。

2 zk自身组成一个集群，2n+1个服务允许n个失效。有两个角色，一个是leader，负责写服务和数据同步，其余的是follower，提供读取服务，
leader失效后，会在follower中重新选举出leader。

3 客户端可以连接到每个server，每个server的数据完全相同
  每个follower和leader连接，接受leader的数据更新操作
  server记录事务日志和快照到持久存储
  大多数server可用，整体服务可用

4 zk特点
	顺序一致性：按照客户端发送的请求顺序更新数据
	原子性：更新要么成功，要么失败，不会出现部分更新
	单一性：无论客户端连接到哪个server，都会看到同一个视图
	可靠性：一旦数据更新成功，将一直保持，直到新的更新
	及时性：客户端会在一个确定的时间内得到最新的数据
在保证一致性的基础上，zk的更高级功能的设计和实现也比较容易。如leader选举 队列 和可撤销等机制。

5 zk应用场景
	数据发布和订阅：应用配置集中到节点上，应用启动时主动获取，并在节点上注册一个watcher，每次配置更新都会通知到应用（lion）
	名空间服务：分布式命名服务，创建一个节点后，节点的路径就是全局唯一的，可以作为全局名称使用
	分布式协调/通知：不同的系统监听同一个节点，一旦有了更新，另外一个系统就能收到通知
	分布式锁：zk能保证数据的强一致性，用户在任何时候都可以相信集群中每个节点的数据都是相同的，一个用户创建一个节点作为锁，
		另一个用户检测该节点，如果存在，代表别的用户已经锁住，如果不存在，则可以创建一个节点，代表拥有一个锁。
	集群管理：每个加入集群的机器都创建一个节点，写入自己的状态，监控父节点的用户会收到通知 ，进行相应的处理，离开时删除节点，
	监控父节点的用户同样会收到通知。
	
	负载均衡：指的是软负载均衡。比如kafka。
	
	
6 分布式队列实现
	在传统的单线程程序中，使用队列数据结构，用来在多个线程之间共享或者传递数据。
	在分布式环境中，需要分布式队列，用来实现 跨进程 跨网络 跨主机的数据共享和数据传递
	zk通过顺序节点实现分布式队列。生产者在zk某个节点下创建顺序节点来存储数据，消费者通过读取这些顺序节点来消费数据
	
	zk队列有两种类型：同步队列（当所有成员都到齐时，队列才可用，否则一直等待所有成员到达）
					  队列按照FIFO方式进行入队和出队操作，如生产者消费者
	同步队列实现方式：创建父节点/root，每个成员都监视/root/start节点是否被创建，然后每个成员都加入这个队列。加入队列的方式就是
	在创建/root/memeber_顺序节点，然后每个成员获取/root下所有子节点，判断成员的个数是否达到要求，如果是，则创建/root/start节点，
	这时会触发事件监听，这样每个成员就会被唤醒开始执行，否则，成员则就等待。(wait notify)
	
	FIFO队列：在特定目录下创建顺序节点/queue_，这样就能保证加入队列的所有成员都是有编号的，出队列时，通过getChildren方法
	返回当前所有队列的元素，然后消费其中最小的一个，这样就能保证FIFO。
	

7 zk不适合做队列
	zk有1MB的传输限制，也就是说znode必须相对较小，而队列包含很多消息，比较大
	如果有多个节点时，zk启动就比较慢，而是用queue会导致出现很多znode
	当大量的包含成千上万的子节点的znode时，zk性能不好
	zk的数据库完全放在内存中。大量的queue意味着会占用很多内存空间。

8 zk同步数据流程
	client连接某个follower，向follower中写数据，该follower会把这个请求传递给leader，然后leader对所有follower分发这个写的请求，
	follower在收到这个写请求后，会向leader回复ack，leader收到ack之后，会向所有follower发送commit请求，这时follower就会写进去。

9 zk提供的一致性？？
	通过同步，zk可以提供强一致性服务，在分区容错性和可用性做了一定这种，和cap理论符合。
	实际上zk提供的是单调一致性：
		假设有2n+1个server，在同步流程中，leader向follower同步数据，当同步完成的follower数量大于n+1时，同步流程结束，系统可以接受
		client的请求。如果client连接的是并非同步完成的follower，那么得到的并不是最新的数据，但是可以保证单调性。
		follower接收写请求后，转发给leader处理，leader完成两阶段提交的机制。向所有server发起提案，当提案超过获取半数的server认同后，将对整个
		集群进行同步，超过半数的server同步完成后，该写请求完成。如果client连接的并非同步完成的follower，那么得到的并非最新的数据，但可以保证单调性。
	利用zk提供的强一致性，能够保证在分布式高并发情况下节点创建的全局唯一性。即同时有多个客户端请求创建某节点时，最终一定只有一个客户端
	请求能够创建成功。
	
10 zk角色
	leader：负责投票的发起和决议，更新系统状态
	follower： 接收客户端请求并返回结果，在选主过程中与参与投票（与leader连接，心跳）
	observer：接收客户端连接，将写请求转发给leader，不参与投票，只同步leader状态。增加observer是为了扩展系统，提高读取速度。
	client：请求发起方。
	随着zk集群节点越来越多，读的吞吐能力和响应能力扩展性好，但是写，随着机器增多，吞吐能力下降，这也就是建立observer的原因。而响应能力取决于具体实现，
	是延迟复制保证最终一致性还是立即复制快速响应。节点多了之后，会放大网络的问题，leader选举也会很耗时的。
	
11 zk工作原理
	Zookeeper 的核心是原子广播，这个机制保证了各个Server之间的同步。实现这个机制的协议叫做Zab协议。Zab协议有两种模式，它们分别是恢复模式（选主）和广播模式（同步）。
当服务启动或者在领导者崩溃后，Zab就进入了恢复模式，当领导者被选举出来，且大多数Server完成了和 leader的状态同步以后，恢复模式就结束了。状态同步保证了leader和Server具有相同的系统状态。 
为了保证事务的顺序一致性，zookeeper采用了递增的事务id号（zxid）来标识事务。所有的提议（proposal）都在被提出的时候加上了zxid。实现中zxid是一个64位的数字，
它高32位是epoch用来标识leader关系是否改变，每次一个leader被选出来，它都会有一个新的epoch，标识当前属于那个leader的统治时期。低32位用于递增计数。

12 zk可以作为分布式存储么？
	不可用。zk主要是作为分布式协调服务的，而且节点默认只有1mb的存储空间。当向zk写数据时，必须向集群中过半的节点写入完成才算完成，
	性能肯定差。而且节点数不宜过多，否则zk读写性能会下降很快。

13 持久化
	leader节点在收到数据变更请求后，首先将变更写入本地磁盘，以作恢复使用，当所有写请求持久化到磁盘后，才会将变更应用到内存中。

14 会话状态？
	建立连接时，会话状态为connecting，当连接成功建立后，会话状态为connected。在会话过程中，正常时，会话状态
	只能是connecting或者connected。如果在会话过程中连接断开，则变为closed状态。
	当客户端失去了和zk服务端的连接，状态会变为connecting，并且尝试寻找其他的server，如果能够找到或者重新连接到之前的server，并确认
	这个session有效，则会转移至状态connected，否则会定义这个session失效，并转移至closed。（也可以显示关闭session）
	只有zk集群负责定义session失效，而不是客户端，客户端不能定义session失效，直至听到zk session超时信息。然后客户端可以选择主动关闭
	这个session。
	当尝试连接另外一个服务器时，要求这个server的zk状态至少要和客户端已经观察到最近的zk状态是一样新的。客户端不能连接到一个这样的服务器，它没有看到
	客户端可能已经看到的更新。zk通过在服务总排序更新操作来决定新鲜程度。每一个zk布局状态的改动操作相对于所有它执行的更新操作都是全序的。
	所以如果一个客户端已经在位置i观察到一个更新，它不能连接一个仅看到i'<i的服务器。在zk实现中，系统分配给每个更新操作的一个事务id来建立这个顺序。
	http://blog.csdn.net/jeff_fangji/article/details/43916359
	
	client和zk server建立连接后，就会触发session的创建。follower会将创建session的请求交付给leader，leader收到请求后，会发起一个创建session的提案，
	如果表决成功（多数派），最终所有的server都会在内存中创建相同的session。在创建session时，会将session持久化，这样在server重启之后，仍然可拿到这个session。
	follower会将自己维护的session信息包括过期时间发送给leader，由leader决定是否过期以及发送closeSession。可见，session的状态完全由leader保持。
	leader会周期性检查全局的session列表，是否有过期，如果有，则将会所有的follower发送closeSession提议，follower在接收到提议后，将session删除。
	http://blog.csdn.net/shift_alt_ctrl/article/details/69944138
	
15 zk注意事项
	客户端连接到zk后，会维持一个tcp连接。在connected状态下，客户端设置了某个znode的watch监听器，可以收到来自该节点变更的通知。
	如果网络异常，客户端断开了与zk的连接，那在断开的过程中，是无法收到watch事件通知的。
	
	如果java堆内存设置不合理，会导致zk内存不足，那么就会在内存与文件系统之间进行数据交换，导致zk性能下降，从而影响应用程序。
	为了避免交换问题的出现，要设置足够的java堆内存，同时减少操作系统和cache使用的内存，尽量避免在内存与文件系统之间发生数据
	交换，或者可以将交换限制在一定的范围之内。
	
	事务日志存储设备性能：zk会将同步事务到存储设备，如果存储设备不是专用的，而是和其他IO密集型应用共享同一磁盘，会导致
	zk效率。因为客户端请求znode数据变更而发生的事务，zk会在响应之前将事务日志写入存储设备，如果存储设备是专用的，那么性能会很大提升
	
	zk设计初衷是存储少量的同步数据，如果存储了大量数据，导致zk每次节点发生变更时，需要将事务写入存储设备，同时还需要在集群内部
	复制传播，这将导致延迟和性能问题。zk中存储的是元数据的信息。
	
16 zk数据存储
	1）内存数据 zk的数据模型是树结构，在内存数据库中存储了整棵树的内容，包括节点的路径 节点数据 ACL信息，zk会定时将这个数据存储到磁盘上
		DataTree:树结构，代表内存中一份完整的数据。
		DataNode：数据存储的最小单元，包含 节点的数据 ACL 节点状态 父节点引用和子节点列表
		ZKDatabase：内存数据库，保存zk的所有会话 datatree存储和事务日志。zkdatabase会定时向磁盘dump快照数据，同时zk启动时，会通过磁盘的事务日志和快照文件恢复成一个完整的内存数据库。
		
	http://www.cnblogs.com/hehheai/p/6506835.html
	
17 zk session
	clienth和session之间通信，需要创建一个session。session有超时时间，zk会将session信息持久化，所以在session超时之前，client和
	zk server的连接可以在各个server之间透明的移动。如果client和zk通信足够频繁，session的维护就不需要其他额外的信息。否则，
	client会每t/3ms发送一次心跳给server，如果client在2t/3ms没收到来自server的心跳回应，就会切换到一个新的server上。t是session超时时间。

18 读写
	zk集群中，可以从任意一个server中读数据，这保证了zk有较好的读性能。写的请求会先转发到leader，由leader原子广播，将请求广播给所有的follower，
	leader收到过半以上的写成功的ack后，就认为成功了，就会将该写进行持久化，并告诉客户端写成功了。

19 WAL和snapshot
	（WAL：从数据库原理而言，实现的是redo日志模式。即修改数据库时，不是直接修改数据库内容，而是将修改完的数据写入日志中，并同步到磁盘上，这样对其他
	读进程没有影响。如果数据库崩溃，重启后扫描日志文件，然后更新到数据库。为了提高效率，wal日志提供checkpoint操作，来定时进行数据更新操作）
	http://blog.csdn.net/kobejayandy/article/details/50885693）
	WAL：write-ahead log，预写日志。对于每个更新操作，zk会先写wal，然后再对内存中的数据做更新，然后向client通知更新结果。zk会定期将内存中的目录树
	进行snapshot，持久化到磁盘。这样做的目的：一是持久化到磁盘 二是加快重启后的恢复速度，如果全部通过replay wal的形式恢复，比较慢
	
20 FIFIO
	对于每个client来说，所有操作都是遵循fifo顺序。由以下保证：一是zk client和server之间的通信基于tcp，tcp保证了传输的有序性。
	二是server执行客户端请求也是按照fifo顺序的。
	
21 线性化
	在zk中，所有更新操作都有严格的偏序关系，更新操作都是串行执行的，这是保证zk功能正确性的关键。
	
22 简单互斥锁实现
	多个进程尝试去在指定的目录下去创建一个临时性(Ephemeral)结点 /locks/my_lock
	ZooKeeper能保证，只会有一个进程成功创建该结点，创建结点成功的进程就是抢到锁的进程，假设该进程为A
	其它进程都对/locks/my_lock进行Watch
	当A进程不再需要锁，可以显式删除/locks/my_lock释放锁；或者是A进程宕机后Session超时，ZooKeeper系统自动删除/locks/my_lock结点释放锁。
	此时，其它进程就会收到ZooKeeper的通知，并尝试去创建/locks/my_lock抢锁，如此循环反复

23 互斥锁实现
	上一种方式每次抢锁都会有大量的进程去竞争，会造成羊群效应。改进如下：
	每个进程都在ZooKeeper上创建一个临时的顺序结点(Ephemeral Sequential) /locks/lock_${seq}
	${seq}最小的为当前的持锁者(${seq}是ZooKeeper生成的Sequenctial Number)
	其它进程都对只watch比它次小的进程对应的结点，比如2 watch 1, 3 watch 2, 以此类推
	当前持锁者释放锁后，比它次大的进程就会收到ZooKeeper的通知，它成为新的持锁者，如此循环反复

24 屏障（Barrier）
	客户端需要等待多个进程完成各自的任务后，然后才能继续向下执行。
	Client在ZooKeeper上创建屏障结点/barrier/my_barrier，并启动执行各个任务的进程
	Client通过exist()来Watch /barrier/my_barrier结点
	每个任务进程在完成任务后，去检查是否达到指定的条件，如果没达到就啥也不做，如果达到了就把/barrier/my_barrier结点删除
	Client收到/barrier/my_barrier被删除的通知，屏障消失，继续下一步任务
	
25 双屏障
	可以用来同步一个任务的开始和结束，当有足够多的进程进入屏障后，才开始执行任务；当所有进程都执行完各自的任务后，屏障才撤销。
	进入屏障：
	Client Watch /barrier/ready结点, 通过判断该结点是否存在来决定是否启动任务
	每个任务进程进入屏障时创建一个临时结点/barrier/process/${process_id}，然后检查进入屏障的结点数是否达到指定的值，如果达到了指定的值，就创建一个/barrier/ready结点，否则继续等待
	Client收到/barrier/ready创建的通知，就启动任务执行过程
	离开屏障：
	Client Watch /barrier/process，如果其没有子结点，就可以认为任务执行结束，可以离开屏障
	每个任务进程执行任务结束后，都需要删除自己对应的结点/barrier/process/${process_id}

26 读写锁
	多个读可以并发执行，读写互斥 写写互斥
	每个进程在zk上创建一个临时顺序节点
	序号最小的一个或者多个节点为当前持锁者，多个是因为读并发
	需要写锁的进程，会判断自己是不是序号最小的，如果不是，需要watch比它次小的进程对应的点
	需要读锁的进程，会watch比它小的最后一个写进程对应的节点
	当前节点释放锁后，所有watch该节点的进程就会被通知，它们成为新的持锁者，如此循环往复
	

http://www.cnblogs.com/onetwo/p/6420062.html
http://wely.iteye.com/blog/2362118